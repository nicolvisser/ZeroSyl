{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tgt\n",
    "import torch\n",
    "from IPython.display import HTML, Audio, display\n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n",
    "from torchcodec.decoders import AudioDecoder\n",
    "\n",
    "from zerosyl.model import ZeroSylBase, ZeroSylDiscrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41abb01",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_mat(features: torch.Tensor) -> np.array:\n",
    "    features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    similarity_matrix = features @ features.T\n",
    "    similarity_matrix = similarity_matrix.cpu().numpy()\n",
    "    return similarity_matrix\n",
    "\n",
    "def plot_sim_mat(similarity_matrix: np.ndarray, melspec: torch.Tensor, textgrid: tgt.TextGrid) -> None:\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        figsize=(8, 8),\n",
    "        gridspec_kw={\"height_ratios\": [1, 8], \"width_ratios\": [1, 8]},\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "\n",
    "    fontsize = 6\n",
    "\n",
    "    # --- top left ---\n",
    "    axes[0][0].axis(\"off\")\n",
    "\n",
    "    # --- Top right ---\n",
    "    ax_tr = axes[0][1]\n",
    "    ax_tr.imshow(melspec.cpu().numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "\n",
    "    xticks = []\n",
    "    xtickslabels = []\n",
    "    for interval in textgrid.get_tier_by_name(\"syllables\"):\n",
    "        x1 = interval.start_time * 50\n",
    "        x2 = interval.end_time * 50\n",
    "        xticks.append((x1 + x2) / 2)\n",
    "        xtickslabels.append(interval.text)\n",
    "        ax_tr.axvline(x1, color=\"white\")\n",
    "        ax_tr.axvline(x2, color=\"white\")\n",
    "    ax_tr.set_xticks(xticks)\n",
    "    ax_tr.set_xticklabels(xtickslabels, rotation=90, fontsize=fontsize)\n",
    "    ax_tr.xaxis.set_ticks_position(\"top\")\n",
    "    ax_tr.xaxis.set_label_position(\"top\")\n",
    "    ax_tr.get_yaxis().set_visible(False)\n",
    "    ax_tr.set_xlim(0, textgrid.end_time * 50)\n",
    "\n",
    "    # --- Bottom left ---\n",
    "    ax_bl = axes[1][0]\n",
    "    ax_bl.imshow(melspec.T.flip(1), aspect=\"auto\", origin=\"upper\")\n",
    "    yticks = []\n",
    "    ytickslabels = []\n",
    "    for interval in textgrid.get_tier_by_name(\"syllables\"):\n",
    "        y1 = interval.start_time * 50\n",
    "        y2 = interval.end_time * 50\n",
    "        yticks.append((y1 + y2) / 2)\n",
    "        ytickslabels.append(interval.text)\n",
    "        ax_bl.axhline(y1, color=\"white\")\n",
    "        ax_bl.axhline(y2, color=\"white\")\n",
    "    ax_bl.set_yticks(yticks)\n",
    "    ax_bl.set_yticklabels(ytickslabels, fontsize=fontsize)\n",
    "    ax_bl.get_xaxis().set_visible(False)\n",
    "    ax_tr.set_xlim(0, textgrid.end_time * 50)\n",
    "\n",
    "    # --- Bottom right ---\n",
    "    ab_br = axes[1][1]\n",
    "    im = ab_br.imshow(similarity_matrix, aspect=\"equal\", origin=\"upper\", vmin=0, vmax=1)\n",
    "    ab_br.axis(\"off\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637fcc9",
   "metadata": {},
   "source": [
    "Load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms_dir = Path(\"data/waveforms/LibriSpeech\")\n",
    "alignments_dir = Path(\"data/alignments/LibriSpeech\")\n",
    "STEM = \"174-50561-0013\"\n",
    "\n",
    "\n",
    "if waveforms_dir.exists() and alignments_dir.exists():\n",
    "    wav_path = next(waveforms_dir.rglob(f\"{STEM}.flac\"))\n",
    "    textgrid_path = next(alignments_dir.rglob(f\"{STEM}.TextGrid\"))\n",
    "else:\n",
    "    # else revert to the sample that is stored in the repository\n",
    "    wav_path = \"data/sample.flac\"\n",
    "    textgrid_path = \"data/sample.TextGrid\"\n",
    "\n",
    "textgrid = tgt.read_textgrid(textgrid_path, include_empty_intervals=False)\n",
    "\n",
    "\n",
    "decoder = AudioDecoder(wav_path, sample_rate=16000, num_channels=1)\n",
    "audio = decoder.get_all_samples()\n",
    "wav = audio.data.cuda()\n",
    "\n",
    "tMelSpectrogram = MelSpectrogram(16000, 1024, 400, 320, n_mels=100)\n",
    "tAmplitudeToDB = AmplitudeToDB(top_db=80)\n",
    "\n",
    "melspec = tAmplitudeToDB(tMelSpectrogram(audio.data))[0]\n",
    "\n",
    "display(Audio(wav_path))\n",
    "\n",
    "transcription = \" \".join([interval.text for interval in textgrid.get_tier_by_name(\"words\")])\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd980e",
   "metadata": {},
   "source": [
    "## 1. WavLM framewise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589d3ed",
   "metadata": {},
   "source": [
    "We start by initializing ZeroSylBase from the official WavLM Large checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37891b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ZeroSylBase.from_pretrained_checkpoint(\"checkpoints/WavLM-Large.pt\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68815afc",
   "metadata": {},
   "source": [
    "ZeroSylBase extends WavLM, so we can then extract framewise features like we would normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = model.extract_features(wav, output_layer=None)\n",
    "    features = features.squeeze(0).cpu()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fe9f6",
   "metadata": {},
   "source": [
    "When we visualize the cosine similary of WavLM features we can see repeating sound patterns at repeating words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc873049",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sim_mat = cosine_sim_mat(features)\n",
    "plot_sim_mat(features_sim_mat, melspec, textgrid);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be5e66",
   "metadata": {},
   "source": [
    "## 2. Meanpooled WavLM features within the detected boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f092d5",
   "metadata": {},
   "source": [
    "In `demo-detect-boundaries.ipynb` we showed how we can extract boundaries by doing prominence based segmentation on layer 13 of WavLM.\n",
    "\n",
    "With the ZeroSylBase class we can access these boundaries like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c72676",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = model.boundaries(wav)\n",
    "print(boundaries)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4), constrained_layout=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "xticks = []\n",
    "xtickslabels = []\n",
    "for interval in textgrid.get_tier_by_name(\"syllables\"):\n",
    "    x1 = interval.start_time * 50\n",
    "    x2 = interval.end_time * 50\n",
    "    xticks.append((x1 + x2) / 2)\n",
    "    xtickslabels.append(interval.text)\n",
    "    plt.axvline(x1, color=\"white\")\n",
    "    plt.axvline(x2, color=\"white\")\n",
    "plt.xticks(xticks, xtickslabels, rotation=90)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.gca().xaxis.set_ticks_position(\"top\")\n",
    "plt.gca().xaxis.set_label_position(\"top\")\n",
    "plt.title(\"Syllables from forced alignments\")\n",
    "plt.xlim(0, textgrid.end_time * 50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "for t in boundaries:\n",
    "    plt.axvline(t * 50, color=\"white\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"ZeroSyl-Base boundaries\")\n",
    "plt.xlim(0, textgrid.end_time * 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4768a2d",
   "metadata": {},
   "source": [
    "We can also meanpool the output embeddings within the segments. This gives us a single continuous embedding for each syllable-like segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c793612",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, starts, ends = model.segment(wav)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(starts)\n",
    "print(ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa757cd",
   "metadata": {},
   "source": [
    "When we visualize the similarity of the meanpooled embeddings, we can easily identify repeating syllables:\n",
    " - `AE` and `P-AH-L` in \"apple\"\n",
    " - `L-EY` in \"lady\"\n",
    " - `D-IY` in \"shady and \"lady\"\n",
    "\n",
    "\n",
    "However we also see high similarity between certain syllables that sound similar, but are distinct:\n",
    " - `N-AW` and `B-AW`\n",
    " - `SH-EY` and `L-EY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_50Hz = model.framewise_meanpooled_embeddings(wav) # <- convenient way to access the framewise (duplicated) embeddings\n",
    "embeddings_sim_mat = cosine_sim_mat(embeddings_50Hz)\n",
    "fig_unboosted = plot_sim_mat(embeddings_sim_mat, melspec, textgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707298bd",
   "metadata": {},
   "source": [
    "We then clustered these embeddings such that we can assign pseudo-labels (discrete tokens) to each syllable-like segments.\n",
    "\n",
    "To obtain the discrete tokens, we load ZeroSylDiscrete that includes a K-means codebook.\n",
    "\n",
    "The K-means model was trained with cosine distance on the normalized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_model = ZeroSylDiscrete.from_pretrained_checkpoint(\n",
    "    \"checkpoints/WavLM-Large.pt\", \"checkpoints/km10000-centroids-v020.pt\"\n",
    ").cuda()\n",
    "tokens, starts, ends = discrete_model.tokenize(wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36379ab5",
   "metadata": {},
   "source": [
    "When we look at the cluster IDs for each of the four segments corresponding to `L-EY`,\n",
    "we see `[3051, 3051, 3051, 3051]`. Great! ðŸ¥³\n",
    "\n",
    "But when we look at other repeating syllables like D-IY, we see `[1905, 3056, 1905, 1905, 7992]`. Less great. ðŸ¥²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), constrained_layout=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "xticks = []\n",
    "xtickslabels = []\n",
    "for interval in textgrid.get_tier_by_name(\"syllables\"):\n",
    "    x1 = interval.start_time * 50\n",
    "    x2 = interval.end_time * 50\n",
    "    xticks.append((x1 + x2) / 2)\n",
    "    xtickslabels.append(interval.text)\n",
    "    plt.axvline(x1, color=\"white\")\n",
    "    plt.axvline(x2, color=\"white\")\n",
    "plt.xticks(xticks, xtickslabels, rotation=90)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.gca().xaxis.set_ticks_position(\"top\")\n",
    "plt.gca().xaxis.set_label_position(\"top\")\n",
    "plt.title(\"Syllables from forced alignments\")\n",
    "plt.xlim(0, textgrid.end_time * 50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "xticks = []\n",
    "xtickslabels = []\n",
    "for token, start, end in zip(tokens, starts, ends):\n",
    "    x1 = start.item()\n",
    "    x2 = end.item()\n",
    "    xticks.append((x1 + x2) / 2)\n",
    "    xtickslabels.append(str(token.item()))\n",
    "    plt.axvline(x1, color=\"white\")\n",
    "    plt.axvline(x2, color=\"white\")\n",
    "plt.xticks(xticks, xtickslabels, rotation=90)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "plt.title(\"ZeroSyl-Discrete boundaries and tokens\")\n",
    "plt.xlim(0, textgrid.end_time * 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7b438",
   "metadata": {},
   "source": [
    "Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c147d",
   "metadata": {},
   "source": [
    "## 3. Boosted framewise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48337ff",
   "metadata": {},
   "source": [
    "We now turn to a method inspired by SyllableLM and Sylber. In their work, the continue training the SSL model to predict the continuous meanpooled representations. SyllableLM called this model Syl**Boost**.\n",
    "\n",
    "So we train out own boosting model but continuing the pretraining of WavLM.\n",
    "Our training objective is slightly different though: Instead of predicting the continuous representations, we predict the discrete cluster ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_model = ZeroSylBase.from_pretrained_checkpoint(\"checkpoints/zerosyl-boost-v020-step-5000.pt\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e593d",
   "metadata": {},
   "source": [
    "Now when extracting features, the features in the final layer are much more piecewise-constant within the boundaries.\n",
    "\n",
    "Unlike in Section 2 above, these are not yet meanpooled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    boosted_features, _ = boosted_model.extract_features(wav, output_layer=None)\n",
    "    boosted_features = boosted_features.squeeze(0).cpu()\n",
    "boosted_features_sim_mat = cosine_sim_mat(boosted_features)\n",
    "plot_sim_mat(boosted_features_sim_mat, melspec, textgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e8779",
   "metadata": {},
   "source": [
    "## 4. Boosted features meanpooled within the detected boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27911c",
   "metadata": {},
   "source": [
    "Now we can go ahead and meanpool the boosted features within the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_embeddings, starts, ends = boosted_model.segment(wav)\n",
    "boosted_embeddings_50Hz = boosted_model.framewise_meanpooled_embeddings(wav) # <- convenient way to access the framewise (duplicated) embeddings\n",
    "boosted_embeddings_sim_mat = cosine_sim_mat(boosted_embeddings_50Hz)\n",
    "fig_boosted = plot_sim_mat(boosted_embeddings_sim_mat, melspec, textgrid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf8346",
   "metadata": {},
   "source": [
    "This looks very similar to the figure in Section 3. So we will need to compare side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f7e18",
   "metadata": {},
   "source": [
    "## Compare unboosted (left) and boosted (right) embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fig_to_base64(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    return base64.b64encode(buf.read()).decode('utf-8')\n",
    "\n",
    "# Convert both figs to base64\n",
    "b64_fig1 = fig_to_base64(fig_unboosted)\n",
    "b64_fig2 = fig_to_base64(fig_boosted)\n",
    "\n",
    "# Display side-by-side\n",
    "html = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: center; gap: 10px;\">\n",
    "  <img src=\"data:image/png;base64,{b64_fig1}\" style=\"max-width: 45%; height: auto;\"/>\n",
    "  <img src=\"data:image/png;base64,{b64_fig2}\" style=\"max-width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa894d",
   "metadata": {},
   "source": [
    "The differences are subtle, but boosting seems better:\n",
    "\n",
    "1. The boosted embeddings (on the right) are generally less similar when they are not the same syllable.\n",
    "    - (there is much more dark purple in the figure on the right)\n",
    "2. On the left plot, the syllable `SH-EY` is similar to all the `D-IY` syllables that follow (turquoise shade)\n",
    "    - on the right plot, these similarities are lower (towards a darker blue)\n",
    "    - So, SYLLABLES THAT ARE NOT SIMILAR APPEAR LESS SIMILAR\n",
    "3. On the left plot the repeated syllables `AE` and `P-AH-L` in \"apple\" is a green color (quite high similarity)\n",
    "    - on the right these are as yellow (similar) as we can get.\n",
    "    - So, SYLLABLES THAT ARE SIMILAR APPEAR MORE SIMILAR\n",
    "\n",
    "There are several more such examples in the plot.\n",
    "\n",
    "But we do not yet know if boosting really helps. We will need to cluster and look at the purity and normalized mutual information metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826744",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zerosyl-Suft6ghr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
