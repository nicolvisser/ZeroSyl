{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b090c4f",
   "metadata": {},
   "source": [
    "# Explainer: ZeroSyl's training free boundary detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b7481",
   "metadata": {},
   "source": [
    "This notebook lays out the steps in our method in an accessible way. This logic is also implemented in classes zerosyl/zerosyl.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94f884",
   "metadata": {},
   "source": [
    "Install ZeroSyl and download sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb431f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install zerosyl\n",
    "!wget https://storage.googleapis.com/zerospeech-checkpoints/5895-34629-0010.flac\n",
    "!wget https://storage.googleapis.com/zerospeech-checkpoints/5895-34629-0010.TextGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eeba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "from scipy.cluster.hierarchy import cut_tree, linkage\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from zerosyl import WavLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9ad25",
   "metadata": {},
   "source": [
    "Load WavLM Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavlm = WavLM.from_remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6f2c",
   "metadata": {},
   "source": [
    "Load a waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e58ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load(\"5895-34629-0010.flac\")\n",
    "\n",
    "display(Audio(wav, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4bd3e",
   "metadata": {},
   "source": [
    "Preprocess the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loudness normalization\n",
    "wav = torch.nn.functional.layer_norm(wav, wav.shape)\n",
    "# zero-pad such that the output features will be perfectly aligned with 20ms intervals\n",
    "wav = torch.nn.functional.pad(wav, ((400 - 320) // 2, (400 - 320) // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce9d69",
   "metadata": {},
   "source": [
    "Extract boundary features from layer 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    boundary_features, _ = wavlm.extract_features(wav, output_layer=13)\n",
    "\n",
    "boundary_features = boundary_features.squeeze(0).cpu().numpy()\n",
    "\n",
    "print(boundary_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6048633",
   "metadata": {},
   "source": [
    "Perform boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46909305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the L2 norm signal\n",
    "norms = np.linalg.norm(boundary_features, axis=-1)\n",
    "\n",
    "# normalize the norm signal\n",
    "norms = (norms - norms.mean()) / norms.std()\n",
    "\n",
    "# smooth the L2 norm signal\n",
    "kernel = np.ones(3) / 3\n",
    "pad_len = 3 // 2\n",
    "norms_padded = np.pad(norms, (pad_len, pad_len), mode=\"edge\")\n",
    "norms_smooth = np.convolve(norms_padded, kernel, mode=\"valid\")\n",
    "\n",
    "# performan prominence based peak detection\n",
    "peaks, _ = find_peaks(norms_smooth, prominence=0.45)\n",
    "\n",
    "# use peaks to detemine boundaries\n",
    "boundaries = [0] + peaks.tolist() + [len(boundary_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904a9c7",
   "metadata": {},
   "source": [
    "Visualize the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c391dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mel spectrogram\n",
    "tMel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_fft=1024,\n",
    "    win_length=400,\n",
    "    hop_length=320,\n",
    ")\n",
    "tDB = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
    "melspec = tDB(tMel(wav.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c801d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = 0, melspec.size(1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "plt.axis(\"off\")\n",
    "for b in boundaries:\n",
    "    plt.axvline(b, c=\"w\")\n",
    "plt.xlim(xmin, xmax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "for b in boundaries:\n",
    "    plt.axvline(b, c=\"gray\", alpha=0.2)\n",
    "\n",
    "plt.plot(norms, label=\"norms\")\n",
    "plt.plot(norms_smooth, label=\"smoothed norms\")\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7d7fe",
   "metadata": {},
   "source": [
    "Listen to the segments (with short silences in between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "listen_samples = []\n",
    "listen_samples.append(np.zeros(8000))\n",
    "for start_frame, end_frame in zip(boundaries[:-1], boundaries[1:]):\n",
    "    listen_samples.append(wav[0, start_frame * 320 : end_frame * 320])\n",
    "    listen_samples.append(np.zeros(8000))\n",
    "listen_samples = np.concat(listen_samples, axis=0)\n",
    "display(Audio(listen_samples, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e68a4a",
   "metadata": {},
   "source": [
    "Extract semantic features from layer 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    semantic_features, _ = wavlm.extract_features(wav, output_layer=22)\n",
    "\n",
    "semantic_features = semantic_features.squeeze(0)\n",
    "\n",
    "print(semantic_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d080a7",
   "metadata": {},
   "source": [
    "Meanpool semantic features within the predicted boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = torch.tensor(boundaries[:-1], device=wav.device)\n",
    "ends = torch.tensor(boundaries[1:], device=wav.device)\n",
    "embeddings = [\n",
    "    semantic_features[start:end].mean(dim=0) for start, end in zip(starts, ends)\n",
    "]\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88185a",
   "metadata": {},
   "source": [
    "K-means discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the K-Means centroids\n",
    "centroids = torch.hub.load_state_dict_from_url(\n",
    "    \"https://storage.googleapis.com/zerospeech-checkpoints/zerosyl-v040-centroids-k-10000.pt\"\n",
    ")\n",
    "# Find the ID of the nearest centroid\n",
    "ids = torch.cdist(embeddings.cpu(), centroids).argmin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22243473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "plt.axis(\"off\")\n",
    "for b1, b2, id in zip(boundaries[:-1], boundaries[1:], ids):\n",
    "    plt.axvline(b1, c=\"w\")\n",
    "    plt.axvline(b2, c=\"w\")\n",
    "    plt.text(\n",
    "        (b1 + b2) / 2, 64, str(id.item()), rotation=90, c=\"w\", ha=\"center\", va=\"center\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e641e",
   "metadata": {},
   "source": [
    "Silences could be fragmented (such as at the start of utterance 5895-34629-0010).\n",
    "\n",
    "After clustering, multiple centroids correspond to silences.\n",
    "We find that language modeling performance improves when these entries are collapsed to a single vocabulary item.\n",
    "We do this in an unsupervised manner by performing hierarchical clustering on the centroids.\n",
    "From informal inspection, we know that the two main branches in agglomerative hierarchical clustering correspond to silences and non-silences, respectively.\n",
    "We pick the smaller branch, which represents silences, and map these items to one vocabulary item.\n",
    "This reduces the vocabulary size from 10\\,000 to 9\\,116."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering\n",
    "linkage_matrix = linkage(\n",
    "    centroids.numpy(), method=\"ward\", metric=\"euclidean\", optimal_ordering=False\n",
    ")\n",
    "# cut dendrogram into 2 main branches\n",
    "silences = cut_tree(linkage_matrix, 2)[:, 0]\n",
    "# the smaller branch should be silences\n",
    "if silences.sum() > (1 - silences).sum():\n",
    "    silences = 1 - silences\n",
    "# to torch tensor\n",
    "silences = torch.from_numpy(silences).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab2856",
   "metadata": {},
   "source": [
    "Now we can identify segments that are silences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silences[ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9845c02",
   "metadata": {},
   "source": [
    "Create a mapping that will merge all the silence (=True) entries while placing all the non-silences at the start of the codebook and placing the single silence at the end of the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = torch.argsort(silences)  # [0,0,....,0,0,1,1,...,1,1]\n",
    "SIL = torch.argmax(silences[order].long()).item()  # position of first 1\n",
    "mapping = torch.empty_like(order)\n",
    "mapping[order] = torch.arange(len(order))\n",
    "mapping[mapping > SIL] = SIL\n",
    "\n",
    "print(f\"The new (single) silence token has the ID: {SIL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651430f7",
   "metadata": {},
   "source": [
    "The new sequence of ids is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45722e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_remapped = mapping[ids]\n",
    "\n",
    "ids_remapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d740f",
   "metadata": {},
   "source": [
    "Now we can collapse consecutive duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_repeated = torch.ones_like(ids_remapped, dtype=torch.bool)\n",
    "not_repeated[1:] = ~torch.logical_and(\n",
    "    ids_remapped[1:] == ids_remapped[:-1], ids_remapped[1:] == SIL\n",
    ")\n",
    "is_end = torch.ones_like(ids_remapped, dtype=torch.bool)\n",
    "is_end[:-1] = ~torch.logical_and(\n",
    "    ids_remapped[1:] == ids_remapped[:-1], ids_remapped[1:] == SIL\n",
    ")\n",
    "starts_merged = starts[not_repeated]\n",
    "ends_merged = ends[is_end]\n",
    "ids_merged = ids_remapped[not_repeated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.imshow(melspec, aspect=\"auto\", origin=\"lower\")\n",
    "plt.axis(\"off\")\n",
    "for start, end, id in zip(starts_merged, ends_merged, ids_merged):\n",
    "    plt.axvline(start, c=\"w\")\n",
    "    plt.axvline(end, c=\"w\")\n",
    "    plt.text(\n",
    "        (start + end) / 2,\n",
    "        64,\n",
    "        str(id.item()),\n",
    "        rotation=90,\n",
    "        c=\"w\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1d6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zerosyl-Ssh5qGkW-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
