{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c994ffd",
   "metadata": {},
   "source": [
    "# Sylber features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3597f",
   "metadata": {},
   "source": [
    "This notebook records how we obtained features from Sylber.\n",
    "\n",
    "Running the code directly in this notebook will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56375074",
   "metadata": {},
   "source": [
    "## Install instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6998b4",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install sylber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2256381",
   "metadata": {},
   "source": [
    "## Extract continuous feature and train K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "from torchcodec.decoders import AudioDecoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sylber import Segmenter\n",
    "\n",
    "segmenter = Segmenter(model_ckpt=\"sylber\", device=\"cuda\")\n",
    "wav_dir = Path(\"/mnt/wsl/hermione/datasets/LibriSpeech\")\n",
    "wav_paths = list(wav_dir.glob(\"train-clean-100/**/*.flac\"))\n",
    "all_embeddings = []\n",
    "for wav_path in tqdm(wav_paths):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    outputs = segmenter(\n",
    "        wav=wav, in_second=False\n",
    "    )  # in_second can be False to output segments in frame numbers.\n",
    "    all_embeddings.append(outputs[\"segment_features\"])\n",
    "\n",
    "all_embeddings = np.concat(all_embeddings, axis=0)\n",
    "\n",
    "num_clusters = 10000\n",
    "\n",
    "num_points, num_dims = all_embeddings.shape\n",
    "print(f\"Found {num_points} points in {num_dims}D\")\n",
    "\n",
    "faiss.normalize_L2(all_embeddings)\n",
    "init_centroids, _ = kmeans_plusplus(all_embeddings, num_clusters)\n",
    "faiss.normalize_L2(init_centroids)\n",
    "\n",
    "step = 0\n",
    "STEPS_PER_CHECKPOINT = 10\n",
    "kmeans = faiss.Kmeans(\n",
    "    d=init_centroids.shape[1],\n",
    "    k=init_centroids.shape[0],\n",
    "    niter=STEPS_PER_CHECKPOINT,\n",
    "    verbose=True,\n",
    "    spherical=True,\n",
    ")\n",
    "\n",
    "while step < 300:\n",
    "    kmeans.train(all_embeddings, init_centroids=init_centroids)\n",
    "    step += STEPS_PER_CHECKPOINT\n",
    "    init_centroids = kmeans.centroids.copy()\n",
    "    np.save(f\"km-centroids-syllablelm-k-{init_centroids.shape[0]}-step-{step}.npy\", kmeans.centroids)\n",
    "    plt.figure()\n",
    "    plt.plot(kmeans.obj)\n",
    "    plt.title(f\"step-{step}\")\n",
    "    plt.savefig(f\"step-{step}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7767ca1",
   "metadata": {},
   "source": [
    "# Extract segments using the discovered centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab865a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "from sylber import Segmenter\n",
    "\n",
    "segmenter = Segmenter(model_ckpt=\"sylber\", device=\"cuda\")\n",
    "wav_dir = Path(\"/mnt/wsl/hermione/datasets/LibriSpeech\")\n",
    "wav_paths = list(wav_dir.glob(\"dev*/**/*.flac\"))\n",
    "out_dir = Path(\"output/segments/sylber-custom-centroids-k-10000-plus-sil/LibriSpeech\")\n",
    "\n",
    "centroids = np.load(\"km-centroids-sylber-k-10000-step-110.npy\")\n",
    "k = centroids.shape[0]\n",
    "faiss.normalize_L2(centroids)\n",
    "index = faiss.IndexFlatIP(centroids.shape[1])\n",
    "index.add(centroids)\n",
    "\n",
    "\n",
    "def reducer(segments, segment):\n",
    "    starts, ends, units = segments  # already added\n",
    "    start, end, unit = segment  # to add\n",
    "\n",
    "    # if there is a gap larger than 7 frames, add an explicit SIL token\n",
    "    # this creates one extra vocabulary item\n",
    "    # see Section A.2.3 on coding efficiency in https://arxiv.org/pdf/2410.07168\n",
    "    if len(ends) == 0 and start > 7:\n",
    "        starts.append(0)\n",
    "        ends.append(start)\n",
    "        units.append(k)\n",
    "    if len(ends) > 0 and start - ends[-1] > 7:\n",
    "        starts.append(ends[-1])\n",
    "        ends.append(start)\n",
    "        units.append(k)\n",
    "\n",
    "    starts.append(start)\n",
    "    ends.append(end)\n",
    "    units.append(unit)\n",
    "    return (starts, ends, units)\n",
    "\n",
    "\n",
    "for wav_path in tqdm(wav_paths):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    outputs = segmenter(wav=wav, in_second=False)\n",
    "    num_hidden_states = len(outputs['hidden_states'])\n",
    "    starts, ends = outputs[\"segments\"].T\n",
    "    embeddings = outputs[\"segment_features\"]\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    _, units = index.search(embeddings, 1)\n",
    "    units = units[:, 0]\n",
    "\n",
    "    starts, ends, units = reduce(reducer, zip(starts, ends, units), ([], [], []))\n",
    "    if num_hidden_states - ends[-1] > 7:\n",
    "        starts.append(ends[-1])\n",
    "        ends.append(num_hidden_states)\n",
    "        units.append(k)\n",
    "\n",
    "    output = np.stack([starts, ends, units], axis=1)\n",
    "    output = torch.from_numpy(output).long()\n",
    "\n",
    "    rel_path = wav_path.relative_to(wav_dir).with_suffix(\".pt\")\n",
    "    out_path = out_dir / rel_path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(output, out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zerosyl-Suft6ghr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
