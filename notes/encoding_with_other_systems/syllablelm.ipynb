{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c994ffd",
   "metadata": {},
   "source": [
    "# SyllableLM features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3597f",
   "metadata": {},
   "source": [
    "This notebook records how we obtained features from SyllableLM.\n",
    "\n",
    "Running the code directly in this notebook will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56375074",
   "metadata": {},
   "source": [
    "## Install instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6998b4",
   "metadata": {},
   "source": [
    "Make sure you are using python 3.9 !\n",
    "\n",
    "```bash\n",
    "python --version\n",
    "```\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/AlanBaade/SyllableLM.git\n",
    "cd SyllableLM\n",
    "python -m venv .venv\n",
    "source ./.venv/bin/activate\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "```\n",
    "\n",
    "or choose one of these if your cuda version is lower than 12.4\n",
    "```\n",
    "# nvidia-smi --version\n",
    "# pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "# pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install numpy omegaconf timm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f620c",
   "metadata": {},
   "source": [
    "## Define feature reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b659ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from syllablelm.data2vec.data.modality import Modality\n",
    "from syllablelm.data2vec.models.data2vec2 import Data2VecMultiModel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "THRESHOLD = 1 / 0.10 / 50.0\n",
    "FULL_MODELS_DICT = {\n",
    "    \"8.33Hz\": {\n",
    "        \"delta\": 0.0033,\n",
    "        \"quantile\": 0.75,\n",
    "    },\n",
    "    \"6.25Hz\": {\n",
    "        \"delta\": 0.0028,\n",
    "        \"quantile\": 0.75,\n",
    "    },\n",
    "    \"5.0Hz\": {\n",
    "        \"delta\": 0.0019,\n",
    "        \"quantile\": 0.75,\n",
    "    },\n",
    "}\n",
    "\n",
    "d2v2_config = SimpleNamespace(\n",
    "    **{\n",
    "        \"_name\": \"data2vec_multi\",\n",
    "        \"loss_beta\": 0.0,\n",
    "        \"loss_scale\": None,\n",
    "        \"depth\": 8,\n",
    "        \"start_drop_path_rate\": 0.0,\n",
    "        \"end_drop_path_rate\": 0.0,\n",
    "        \"num_heads\": 12,\n",
    "        \"norm_eps\": 1e-05,\n",
    "        \"norm_affine\": True,\n",
    "        \"encoder_dropout\": 0.1,\n",
    "        \"post_mlp_drop\": 0.1,\n",
    "        \"attention_dropout\": 0.1,\n",
    "        \"activation_dropout\": 0.0,\n",
    "        \"dropout_input\": 0.0,\n",
    "        \"layerdrop\": 0.05,\n",
    "        \"embed_dim\": 768,\n",
    "        \"mlp_ratio\": 4.0,\n",
    "        \"layer_norm_first\": False,\n",
    "        \"average_top_k_layers\": 8,\n",
    "        \"end_of_block_targets\": False,\n",
    "        \"clone_batch\": 8,\n",
    "        \"layer_norm_target_layer\": False,\n",
    "        \"batch_norm_target_layer\": False,\n",
    "        \"instance_norm_target_layer\": True,\n",
    "        \"instance_norm_targets\": False,\n",
    "        \"layer_norm_targets\": False,\n",
    "        \"ema_decay\": 0.999,\n",
    "        \"ema_same_dtype\": True,\n",
    "        \"log_norms\": True,\n",
    "        \"ema_end_decay\": 0.99999,\n",
    "        \"ema_anneal_end_step\": 75000,\n",
    "        \"ema_encoder_only\": False,\n",
    "        \"max_update\": 400000,\n",
    "        \"modalities\": SimpleNamespace(\n",
    "            **{\n",
    "                \"_name\": None,\n",
    "                \"audio\": SimpleNamespace(\n",
    "                    **{\n",
    "                        \"type\": Modality.AUDIO,\n",
    "                        \"prenet_depth\": 4,\n",
    "                        \"prenet_layerdrop\": 0.05,\n",
    "                        \"prenet_dropout\": 0.1,\n",
    "                        \"start_drop_path_rate\": 0.0,\n",
    "                        \"end_drop_path_rate\": 0.0,\n",
    "                        \"num_extra_tokens\": 0,\n",
    "                        \"init_extra_token_zero\": True,\n",
    "                        \"mask_noise_std\": 0.01,\n",
    "                        \"mask_prob_min\": None,\n",
    "                        \"mask_prob\": 0.5,\n",
    "                        \"inverse_mask\": False,\n",
    "                        \"mask_prob_adjust\": 0.05,\n",
    "                        \"keep_masked_pct\": 0.0,\n",
    "                        \"mask_length\": 5,\n",
    "                        \"add_masks\": False,\n",
    "                        \"remove_masks\": False,\n",
    "                        \"mask_dropout\": 0.0,\n",
    "                        \"encoder_zero_mask\": True,\n",
    "                        \"mask_channel_prob\": 0.0,\n",
    "                        \"mask_channel_length\": 64,\n",
    "                        \"ema_local_encoder\": False,\n",
    "                        \"local_grad_mult\": 1.0,\n",
    "                        \"use_alibi_encoder\": True,\n",
    "                        \"alibi_scale\": 1.0,\n",
    "                        \"learned_alibi\": False,\n",
    "                        \"alibi_max_pos\": None,\n",
    "                        \"learned_alibi_scale\": True,\n",
    "                        \"learned_alibi_scale_per_head\": True,\n",
    "                        \"learned_alibi_scale_per_layer\": False,\n",
    "                        \"num_alibi_heads\": 12,\n",
    "                        \"model_depth\": 8,\n",
    "                        \"decoder\": SimpleNamespace(\n",
    "                            **{\n",
    "                                \"decoder_dim\": 384,\n",
    "                                \"decoder_groups\": 16,\n",
    "                                \"decoder_kernel\": 7,\n",
    "                                \"decoder_layers\": 4,\n",
    "                                \"input_dropout\": 0.1,\n",
    "                                \"add_positions_masked\": False,\n",
    "                                \"add_positions_all\": False,\n",
    "                                \"decoder_residual\": True,\n",
    "                                \"projection_layers\": 1,\n",
    "                                \"projection_ratio\": 2.0,\n",
    "                                \"channel_mult\": [1, 0.5, 0.25, 0.25, 0.25],\n",
    "                                \"decoder_transformer_layers\": 4,\n",
    "                            }\n",
    "                        ),\n",
    "                        \"extractor_mode\": \"layer_norm\",\n",
    "                        \"feature_encoder_spec\": \"[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]\",\n",
    "                        \"conv_pos_width\": 95,\n",
    "                        \"conv_pos_groups\": 16,\n",
    "                        \"conv_pos_depth\": 5,\n",
    "                        \"conv_pos_pre_ln\": False,\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "        \"shared_decoder\": None,\n",
    "        \"min_target_var\": 0.1,\n",
    "        \"min_pred_var\": 0.01,\n",
    "        \"supported_modality\": Modality.AUDIO,\n",
    "        \"mae_init\": False,\n",
    "        \"seed\": 1,\n",
    "        \"skip_ema\": False,\n",
    "        \"cls_loss\": 0.0,\n",
    "        \"recon_loss\": 0.0,\n",
    "        \"d2v_loss\": 1.0,\n",
    "        \"decoder_group\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "class ApplyKmeans(object):\n",
    "    def __init__(self, km_path):\n",
    "        self.cluster_centers = np.load(km_path)\n",
    "        self.C_np = self.cluster_centers.transpose()\n",
    "        self.Cnorm_np = (self.C_np**2).sum(0, keepdims=True)\n",
    "\n",
    "        self.C = torch.from_numpy(self.C_np)\n",
    "        self.Cnorm = torch.from_numpy(self.Cnorm_np)\n",
    "        self.C = self.C.cuda()\n",
    "        self.Cnorm = self.Cnorm.cuda()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dist = x.pow(2).sum(-1, keepdim=True) - 2 * torch.matmul(x, self.C) + self.Cnorm\n",
    "        return dist.argmin(dim=-1).cpu()\n",
    "\n",
    "\n",
    "# @torch.compile()\n",
    "@torch.inference_mode()\n",
    "def efficient_extraction_dp_helper(x, threshold=THRESHOLD, s=35, min_hop=3):\n",
    "    # x: batch, num feats, dimension feature vector. No support for padding, but setting remaining feats to zeros probably works\n",
    "    # threshold: max hz before search using delta\n",
    "    # s: max size of a chunk (50=1sec). 50 for librispeech, 35 for librilight\n",
    "    # min_hop: min size of a chunk\n",
    "    # Alan: If you have questions, I'm sorry. I forget how this works, too.\n",
    "\n",
    "    b, n, d = x.shape\n",
    "\n",
    "    dists = x.new_full((b, s + 1, n + s), 16384)\n",
    "\n",
    "    rolled = torch.stack(\n",
    "        [torch.roll(x, shifts=-i, dims=-2) for i in range(s)]\n",
    "    ).transpose(0, 1)\n",
    "    rolled_prepend = x[:, :s].unsqueeze(2).repeat(1, 1, s - 1, 1)\n",
    "    arranged = torch.cat([rolled_prepend, rolled], dim=2)\n",
    "\n",
    "    len_indices = torch.arange(s, device=x.device) + 1\n",
    "    dots = arranged.pow(2).mean(dim=-1).cumsum(dim=-2)\n",
    "    middle = (\n",
    "        -1 / len_indices.view(1, -1, 1) * arranged.cumsum(dim=-3).pow(2).mean(dim=-1)\n",
    "    )\n",
    "    outs = dots + middle\n",
    "    outs = torch.cat(\n",
    "        [outs[:, i : i + 1].roll(shifts=-(s - i - 1), dims=2) for i in range(s)], dim=1\n",
    "    )\n",
    "    dists[:, 1:, s:] = outs[:, :, : -(s - 1)]\n",
    "    dists += dists.new_full(dists.shape, 16384).tril(s - 2)\n",
    "    dists = dists.clamp(max=16384)\n",
    "\n",
    "    m = int(threshold * n)\n",
    "    total_dists = x.new_full((b, n + 2), 16384)\n",
    "    total_dists[:, 0] = 0\n",
    "    back = x.new_zeros((b, n + 1, m + 1), dtype=int)\n",
    "    magic_mask = (\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [(j + 1 - k if j + 1 >= k else n + 1) for j in range(n)]\n",
    "                for k in range(min_hop, s + 1)\n",
    "            ],\n",
    "            device=x.device,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "        .expand(b, s + 1 - min_hop, n)\n",
    "    )\n",
    "\n",
    "    for j in range(1, m + 1):\n",
    "        cur_min = torch.min(\n",
    "            total_dists.unsqueeze(1)\n",
    "            .expand(b, s + 1 - min_hop, n + 2)\n",
    "            .gather(2, magic_mask)\n",
    "            + dists[:, min_hop:, s : n + s],\n",
    "            dim=1,\n",
    "        )\n",
    "        total_dists[:, 1:-1] = cur_min.values\n",
    "        back[:, 1 : 1 + n, j] = cur_min.indices + min_hop\n",
    "\n",
    "    return dists, back\n",
    "\n",
    "\n",
    "def get_quantile_borders_helper(\n",
    "    dists, back, n=None, s=None, num_units=None, delta=None, quantile=None\n",
    "):\n",
    "    # Binary search on dp array for the dynamic number of cuts given delta. Section 5.3 of paper.\n",
    "\n",
    "    min_, max_ = num_units // 3, num_units\n",
    "    best_m = min_\n",
    "\n",
    "    while min_ <= max_:\n",
    "        mid_ = (min_ + max_) // 2\n",
    "\n",
    "        q = n\n",
    "        j = mid_\n",
    "        costs = []\n",
    "        while q > 0:\n",
    "            costs.append(dists[back[q, j], q - 1 + s] / back[q, j])\n",
    "            q = q - back[q, j]\n",
    "            j = j - 1\n",
    "        quantile_cost = np.quantile(costs, quantile)\n",
    "\n",
    "        if quantile_cost > delta:\n",
    "            min_ = mid_ + 1\n",
    "            best_m = mid_\n",
    "        else:\n",
    "            max_ = mid_ - 1\n",
    "\n",
    "    q = n\n",
    "    j = best_m\n",
    "    borders = [q]\n",
    "    while q > 0:\n",
    "        q = q - back[q, j]\n",
    "        borders.append(q)\n",
    "        j = j - 1\n",
    "    borders.reverse()\n",
    "\n",
    "    return borders\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def efficient_extraction(\n",
    "    embeddings, threshold=THRESHOLD, s=35, min_hop=3, deltas=None, quantiles=None\n",
    "):\n",
    "    b, n, d = embeddings.shape\n",
    "    x = embeddings.cuda().float()\n",
    "    m = int(threshold * n)\n",
    "    s = min(n, s)\n",
    "\n",
    "    dists, back = efficient_extraction_dp_helper(\n",
    "        x, threshold=threshold, s=s, min_hop=min_hop\n",
    "    )\n",
    "\n",
    "    back = back.cpu().numpy()\n",
    "    dists = dists.cpu().numpy()\n",
    "\n",
    "    batch_outs = [\n",
    "        [\n",
    "            get_quantile_borders_helper(\n",
    "                d_, b_, n=n, s=s, num_units=m, delta=delta, quantile=quantile\n",
    "            )\n",
    "            for d_, b_ in zip(dists, back)\n",
    "        ]\n",
    "        for delta, quantile in zip(deltas, quantiles)\n",
    "    ]\n",
    "\n",
    "    return batch_outs\n",
    "\n",
    "\n",
    "class SylBoostFeatureReader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sylboost_checkpoint,\n",
    "        kmeans_centroids_path,\n",
    "        agglom_indices_path,\n",
    "        model_key,\n",
    "    ):\n",
    "        d2v2_model = Data2VecMultiModel(d2v2_config, [Modality.AUDIO])\n",
    "        d2v2_model = d2v2_model.cuda().eval().half()\n",
    "        state_dict = torch.load(sylboost_checkpoint)\n",
    "        d2v2_model.load_state_dict(\n",
    "            {k[len(\"model.\") :]: v for k, v in state_dict[\"model_seg\"].items()}\n",
    "        )\n",
    "        self.d2v2_model = d2v2_model\n",
    "\n",
    "        self.kmeans_centroids = ApplyKmeans(kmeans_centroids_path)\n",
    "        self.agglom = np.load(agglom_indices_path)\n",
    "        self.model_key = model_key\n",
    "\n",
    "        assert model_key in FULL_MODELS_DICT.keys()\n",
    "        self.delta = FULL_MODELS_DICT[model_key][\"delta\"]\n",
    "        self.quantile = FULL_MODELS_DICT[model_key][\"quantile\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        # Input:\n",
    "        # x : (b, t) batched waveform tensor at 16000Hz.\n",
    "        # Returns:\n",
    "        # features: (b, n, d) raw data2vec2 features\n",
    "        # clusters_with_times: list of length b, each item is clusters and boundaries. Clusters has 3 rows.\n",
    "        #   # 0th row: KMeans+Agglom Cluster. 1st row: start boundary idx (inclusive). 2nd row: end boundary idx (exclusive).\n",
    "\n",
    "        features = self.d2v2_model(\n",
    "            x.half(),\n",
    "            mode=None,\n",
    "            mask=False,\n",
    "            features_only=True,\n",
    "            remove_extra_tokens=True,\n",
    "            out_layer=-2,\n",
    "        )[\"x\"]\n",
    "        result = {\n",
    "            \"features\": features,\n",
    "            \"starts\": [],\n",
    "            \"ends\": [],\n",
    "            \"clusters\": [],\n",
    "        }\n",
    "\n",
    "        # Multiple deltas at once suported (why not) but we just use one\n",
    "        deltas = [self.delta]\n",
    "        quantiles = [self.quantile]\n",
    "        mincut = efficient_extraction(features, deltas=deltas, quantiles=quantiles)[0]\n",
    "\n",
    "        for b_idx, (feats, mincut_boundaries) in enumerate(zip(features, mincut)):\n",
    "            mincut_boundaries = np.array(mincut_boundaries)\n",
    "            meaned_features = torch.stack(\n",
    "                [\n",
    "                    feats[\n",
    "                        mincut_boundaries[idx] + 1 : mincut_boundaries[idx + 1] - 1\n",
    "                    ].mean(dim=0)\n",
    "                    for idx in range(len(mincut_boundaries) - 1)\n",
    "                ]\n",
    "            )  # t,dim\n",
    "            meaned_features = (\n",
    "                meaned_features - meaned_features.mean(dim=-1, keepdim=True)\n",
    "            ) / meaned_features.std(dim=-1, keepdim=True)\n",
    "\n",
    "            clusters = self.agglom[\n",
    "                self.kmeans_centroids(meaned_features.float()).numpy()\n",
    "            ].reshape(-1)\n",
    "\n",
    "            # Sequential Deduplication\n",
    "            not_repeat_mask = ~np.insert((clusters[1:] == clusters[:-1]), 0, 0)\n",
    "            not_repeat_mask_end = ~np.insert(\n",
    "                (clusters[1:] == clusters[:-1]), clusters.shape[0] - 1, 0\n",
    "            )  # RLE\n",
    "\n",
    "            result[\"starts\"].append(mincut_boundaries[:-1][not_repeat_mask])\n",
    "            result[\"ends\"].append(mincut_boundaries[1:][not_repeat_mask_end])\n",
    "            result[\"clusters\"].append(clusters[not_repeat_mask])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1b031",
   "metadata": {},
   "source": [
    "## Extract IDs of SyllableLM's method using Sylboost 6.25 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8aaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "wav_dir = Path(\"/mnt/wsl/hermione/datasets/LibriSpeech\")\n",
    "wav_paths = sorted(wav_dir.glob(\"dev*/**/*.flac\"))\n",
    "out_dir = Path(\"output/segments/syllablelm-625-official-ids/LibriSpeech\")\n",
    "\n",
    "sylboost_reader = SylBoostFeatureReader(\n",
    "    f\"checkpoints/SylBoost_625Hz.pth\",\n",
    "    f\"checkpoints/SylBoost_625Hz_kmeans.npy\",\n",
    "    f\"checkpoints/SylBoost_625Hz_agglom.npy\",\n",
    "    \"6.25Hz\",\n",
    ")\n",
    "\n",
    "for wav_path in tqdm(wav_paths):\n",
    "\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    assert sr == 16000\n",
    "\n",
    "    wav = torch.nn.functional.pad(wav, ((400 - 320) // 2, (400 - 320) // 2))\n",
    "\n",
    "    x = sylboost_reader.forward(wav.cuda().half())\n",
    "\n",
    "    starts = x[\"starts\"][0]\n",
    "    ends = x[\"ends\"][0]\n",
    "    units = x[\"clusters\"][0]\n",
    "    output = np.stack([starts, ends, units], axis=1)\n",
    "    output = torch.from_numpy(output).long()\n",
    "\n",
    "    rel_path = wav_path.relative_to(wav_dir).with_suffix(\".pt\")\n",
    "    out_path = out_dir / rel_path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(output, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f461b",
   "metadata": {},
   "source": [
    "When encoding LibriLight, we use VAD to segment an audiobook into roughly 15 second segments by placing boundaries in the middle of larger silence regions.\n",
    "- therefore the whole LibriLight is segmented CONTIGUOUSLY.\n",
    "\n",
    "Then when extracting SyllableLM tokens, we encode each segment\n",
    "- WITHOUT ZERO PADDING\n",
    "\n",
    "Finally the token sequences for audiobooks are concatenated together.\n",
    "\n",
    "\n",
    "The two points above improve language modeling scores for SyllableLM compared to throwing away silences or padding with zeros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zerosyl-Suft6ghr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
